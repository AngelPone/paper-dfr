\documentclass{article}

\usepackage{amsfonts,amsmath,bm,xcolor}
\title{Discrete Forecast Reconciliation: An Idea}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bpi}{\bm{\pi}}
\author{Anastasios Panagiotelis}

\begin{document}

	\maketitle
	\section{Preliminaries}
	Let $\bY=\left(Y_1,Y_2,\ldots,Y_n\right)'$ be an $n$-vector of discrete random variables. Let $\bY$ be partitioned so that the first $m$ elements are `bottom-level' variables the remaining $(n-m)$ variables are `top-level' series. The `top-level' variables are some deterministic linear function of the `bottom-level' variables, usually aggregates. Let the domain of each element of $\bY$ be given by $\mathcal{D}(Y_i)=\left\{0, 1,2,3,\dots,D_i\right\}$, where $D_i$ is finite.
	
	\section{Incoherent and Incoherent Domain}
	The \textit{incoherent domain} of $\bY$ is given by
	\[
	\hat{\mathcal D}(\bY)=\prod\limits_{i=1}^n\hat{\mathcal D}(\bY_i)=\left\{0, 1,2,\dots,D_1\right\}\times\left\{0,1,2,\dots,D_2\right\}\times\dots\times\left\{0,1,2,\dots,D_n\right\}\,,
	\] 
    where products are set products. The cardinality of the incoherent domain $|\hat{\mathcal D}(\bY)|=\prod\limits_{i=1}^{n} (D_i+1)$ which will be denoted by $q$. The incoherent domain is analogous to $\mathbb{R}^n$ is the continuous case.
    
    The \textit{coherent domain} of $\bY$ is given by a subset of $\tilde{\mathcal D}(\bY)$ for which aggregation constraints hold.  The coherent domain has cardinality $|\tilde{\mathcal D}(\bY)|=\prod\limits_{i=1}^{m} (D_i+1)$. This will be denoted as $r$.  The coherent domain is analogous to the coherent subspace $\mathfrak{s}$ in the continuous case.
    
    \subsubsection*{Example}
    
    Let $Y_1$ and $Y_2$ be binary variables and $Y_3=Y_1+Y_2$. In this case the domain of each variable is
    \begin{align*}
      \mathcal{D}(Y_1)=&\left\{0,1\right\}\\
      \mathcal{D}(Y_2)=&\left\{0,1\right\}\\
      \mathcal{D}(Y_3)=&\left\{0,1,2\right\}\,,
    \end{align*}	
    the incoherent domain is
    \begin{align*}
    \hat{\mathcal D}(\bY)=&\left\{\textcolor{blue}{(0,0,0)'},(0,1,0)',(1,0,0)',(1,1,0)',\right.\\
    &\left.(0,0,1)',\textcolor{blue}{(0,1,1)'},\textcolor{blue}{(1,0,1)'},(1,1,1)',\right.\\
    &\left.(0,0,2)',(0,1,2)',(1,0,2)',\textcolor{blue}{(1,1,2)'}\right\}\,,
    \end{align*}
    and the coherent domain is given by those points for which $y_1+y_2=y_3$, highlighted in blue above, i.e
    \[
        \tilde{\mathcal D}(\bY)=\left\{(0,0,0)',(0,1,1)',(1,0,1)',(1,1,2)'\right\}\,.
    \]
    
    \section{Probabilistic Forecasts}
    Suppose a probabilistic $h$-step ahead forecast is made for $\bY$ at time $t$. Let $\hat{\bpi}^{t+h|t}$ be a $q$-vector of probabilities where each element in $\hat{\bpi}^{t+h|t}$ corresponds to one point in the incoherent domain. Two notational conventions will be used for the elements of $\hat{\bpi}^{t+h|t}$. First, $\hat{\pi}_j^{t+h|t}$ will be used to denote the $j^{th}$ element of $\hat{\bpi}^{t+h|t}$.  Second, $\hat{\pi}_{(y_1 y_2 \dots y_n)}^{t+h|t}$ will be used to denote that the specific element of $\hat{\bpi}^{t+h|t}$, corresponds to the forecast probability that the $\bY$ takes a value $(y_1,y_2,\dots,y_n)'$. The two conventions are linked by a function $\hat{h}:{1,2,\dots,q}\rightarrow\hat{\mathcal{D}}(\bY)$ which maps each index $j$ to a configuration of values that $\bY$ can take. Using the small example in the previous section $\hat{\pi}_1^{t+h|t}=\hat{\pi}_{(000)}^{t+h|t}$, $\hat{\pi}_2^{t+h|t}=\hat{\pi}_{(010)}^{t+h|t}$, etc., and $\hat{h}(1)=(0,0,0)'$, $\hat{h}(2)=(0,1,0)'$, etc.
    
    This probabilistic forecast is incoherent as long as there is some $\hat{\bpi}^{t+h|t}_j\neq 0$ for which $\hat{h}(j)\notin\tilde{\mathcal{D}}(\bY)$, i.e. probability is assigned to some point for which the aggregation constraints do not hold. This can happen quite easily in practice. Suppose probabilistic forecasts are generated for each variable independently and the joint forecast is then constructed assuming independence. Such forecasts will in general be incoherent. For example suppose $Pr(Y_1=0)=0.2$,$Pr(Y_2=1)=0.1$,$Pr(Y_3=0)=0.05$, then under independence $Pr(Y_1=0,Y_2=1,Y_3=0)=0.2\times0.1\times0.05$. We are assigning non-zero probability to an incoherent point.
    
    A coherent probabilistic function can be defined in a similar fashion. Now $\tilde{\bpi}^{t+h|t}$ is an $r$-vector of probabilities where each element in $\tilde{\bpi}^{t+h|t}$ corresponds to one point in the coherent domain. The notation $\tilde{\pi}_k^{t+h|t}$ is used to denote the $k^{th}$ element of $\tilde{\bpi}^{t+h|t}$ and the notation $\tilde{\pi}_{(y_1 y_2 \dots y_n)}^{t+h|t}$ will be used to denote that the specific element of $\tilde{\bpi}^{t+h|t}$, corresponds to the forecast probability that the $\bY$ takes a value $(y_1,y_2,\dots,y_n)'$ where this value must be coherent. The analogue to $\hat{h}(j)$ is a function a function $\tilde{h}:{1,2,\dots,r}\rightarrow\tilde{\mathcal{D}}(\bY)$ which maps each index $k$ to a coherent configuration of values that $\bY$ can take. Using the small example in the previous section $\tilde{\pi}_1^{t+h|t}=\tilde{\pi}_{(000)}^{t+h|t}$, $\tilde{\pi}_2^{t+h|t}=\tilde{\pi}_{(011)}^{t+h|t}$, etc., and $\tilde{h}(1)=(0,0,0)'$, $\tilde{h}(2)=(0,1,1)'$, etc.
    
    \subsubsection*{Example}
    
    Consider the earlier example, with binary $y_1$ and $y_2$ and $y_1+y_2=y_3$. The incoherent probabilistic forecast is given by
    \[
      \hat{\bpi}^{t+h|t}=\begin{pmatrix}
         \hat{\pi}^{t+h|t}_{(000)}\\
         \hat{\pi}^{t+h|t}_{(010)}\\
         \hat{\pi}^{t+h|t}_{(100)}\\
         \hat{\pi}^{t+h|t}_{(110)}\\
         \hat{\pi}^{t+h|t}_{(001)}\\
         \hat{\pi}^{t+h|t}_{(011)}\\
         \hat{\pi}^{t+h|t}_{(101)}\\
         \hat{\pi}^{t+h|t}_{(111)}\\
         \hat{\pi}^{t+h|t}_{(002)}\\
         \hat{\pi}^{t+h|t}_{(012)}\\
         \hat{\pi}^{t+h|t}_{(102)}\\
         \hat{\pi}^{t+h|t}_{(112)}\\
      \end{pmatrix}
    \]
    where the notation $\hat{\pi}^{t+h|t}_{1}$ can be used instead of $\hat{\pi}^{t+h|t}_{(000)}$, $\hat{\pi}^{t+h|t}_{2}$ can be used instead of $\hat{\pi}^{t+h|t}_{(010)}$, etc. Also the function $\hat{h}$ is defined so that $\hat{h}(1)=(0,0,0)'$, $\hat{h}(2)=(0,1,0)'$, etc.
    
    The coherent probabilistic forecast is given by
    \[
    \tilde{\bpi}^{t+h|t}=\begin{pmatrix}
    \tilde{\pi}^{t+h|t}_{(000)}\\
    \tilde{\pi}^{t+h|t}_{(011)}\\
    \tilde{\pi}^{t+h|t}_{(101)}\\
    \tilde{\pi}^{t+h|t}_{(112)}\\
    \end{pmatrix}
    \]
    where the notation $\tilde{\pi}^{t+h|t}_{1}$ can be used instead of $\tilde{\pi}^{t+h|t}_{(000)}$, $\tilde{\pi}^{t+h|t}_{2}$ can be used instead of $\tilde{\pi}^{t+h|t}_{(011)}$, etc. Also the function $\tilde{h}$ is defined so that $\tilde{h}(1)=(0,0,0)'$, $\tilde{h}(2)=(0,1,1)'$, etc. Note that the ordering of the probabilities and the functions $\hat{h}$ and $\tilde{h}$ are not unique although this does not affect the proposed algorithms.
    
    \section{Reconciliation}
    
    The idea is to `move' probability from $\hat{\bpi}$ to $\tilde{\bpi}$ with the superscript $t+h|t$ dropped for convenience.  Let
    
    \[
    \tilde{\bpi}=\bm{A}\hat{\bpi}
    \] 
    
    where $\bm{A}$ is a matrix of assignment weights. Letting, $a_{kj}$ be the element in row $k$ and column $j$ of $\bm{A}$, this is equivalent to
    \[
      \tilde{\pi}_k=\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j
    \]
    for all $k$. Each $a_{kj}$ tells us how much probability is shifted from the (possibly) incoherent point $\hat{h}(j)$ to the coherent point $\tilde{h}(k)$ (or from element $j$ in $\hat{\bpi}$ to element $k$ in $\hat{\bpi}$). We know that $a_{kj}$ must meet the following constraints
    \begin{align*}
    &0\leq a_{kj} \leq 1 \,\forall k, j\\ 
    &\sum\limits_{k=1}^r a_{kj} = 1 \,\forall j 
    \end{align*}
    The first constraint guarantees that the elements of $\tilde{\bpi}$ are between 0 and 1, while the second constraint guarantees that the elements of $\tilde{\bpi}$ sum to 1.
    
    The next thing to consider is what would make a `good' candidate for $\bm{A}$.  One property is that that we move probability from incoherent points to coherent points that are  `nearby' in some sense. Another is that the resulting coherent forecasts perform well.  Since we are using probabilistic forecasts some scoring rule can be used to measure performance.
    
    \subsection{Costs}
    
    The idea of moving probability to nearby coherent points is inspired by the use (and success) of projections in the reconciliation literature for continuous forecasts. We define a `cost' of moving probability from $\hat{\pi}_j\rightarrow\tilde{\pi}_k$ as
    \[
    c_{kj}=||\hat{h}(j)-\tilde{h}(k)||_1\,,
    \]
    where $||.||_1$ is the L1 norm. To illustrate a single cost in the three variable example is given by
    \[
    c_{12}=\left|\left|\begin{pmatrix}0\\1\\0\end{pmatrix}-\begin{pmatrix}0\\0\\0\end{pmatrix}\right|\right|_1=1\,.
    \]
    Note that in contrast to the continuous case the `nearest' coherent point is not unique for example $(0,1,0)'$ is equally distant from $(0,0,0)'$ and $(0,1,1)'$ both of which are coherent. For this reason we cannot base discrete reconciliation on a discrete analogue to projections alone.
    
    \subsection{Brier Score}
    
    Assume that $\hat{\bpi}^{t+h|t}$ are found for $t\in\mathcal{T}_{\textrm{window}}$, were $\mathcal{T}_{\textrm{window}}$ is a rolling window. Also, let $z^{t+h}$ be an $r$-vector with element $k=1$ if $\tilde{h}(k)=\bm{y}^{t+h}$ and $0$ otherwise where $\bm{y}^{t+h}$ is the actual realisation of $Y$ at time $t+h$. The Brier score is  proper scoring rule and can be averaged over the rolling window as
    \begin{align*}
    \textrm{Av. Brier Sc.}=&\frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\tilde{\pi}_k^{t+h|t}-z^{t+h}_k\right)^2\right]\\
    =&\frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j-z^{t+h}_k\right)^2\right]\,.
    \end{align*}
    This is a quadratic function of the $a_{kj}$ with smaller values indicating a better coherent forecast.
    
    \section{The optimisation problem}
    
    Putting everything together, the optimisation problem is to 
    
    \[
    \underset{a_{kj}}{\min} \frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j-z^{t+h}_k\right)^2\right] + \sum\limits_{k=1}^r\sum\limits_{j=1}^q c_{kj}a_{kj}\,
    \]
    subject to
    \begin{align*}
    &0\leq a_{kj}\leq 1\\
    &\sum\limits_{k=1}^r a_{kj} = 1 \,\forall j
    \end{align*}
    This is a fairly standard problem in quadratic programming.

    \subsection{Matrix form of the optimisation problem}

    The optimisation priblem can be formulated into matrix form.
    \[
    \begin{align*}
      \min\limits_{A \in \mathbb{R}^{r\times q}} \frac{1}{|\mathcal{T}_{\textrm{window}}|}\text{vec}(A)' Q \text{vec}(A) &+ \left[\lambda \text{vec}(c)' - \frac{2}{|\mathcal{T}_{\textrm{window}}|}\text{vec}(Z)'D\right]\text{vec}(A) \\&+ \frac{1}{|\mathcal{T}_{\textrm{window}}|} \text{vec}(Z)'\text{vec}(Z)\,
    \end{align*}
    \]
    subject to \[\begin{align*}
      0 \leq A \leq 1\\
      E A = \mathbf{1}
    \end{align*} \\ \]
    where
    \[
    Q = \left[\begin{matrix}
      Q_1  & \\
       &\ddots \\
       &  & Q_r

    \end{matrix}  \right]
    D = \left[\begin{matrix}
      D_1 \\ \vdots \\ D_T
    \end{matrix}\right] 
    D_t = \left[\begin{matrix}
      \hat\pi_{t}' & \\
      & \ddots & \\
      & & \hat\pi_{t}'
    \end{matrix}\right]_{r\times (rq)}
    \]
    and
    \[\textrm{vec}(c) = \left[c_1' \dots c_r'\right]',\]
    \[\textrm{vec}(Z) = \left[Z_1' \dots Z_{|\mathcal{T}_{\textrm{window}}|}'\right]',\]
    \[E = [I_q \quad I_q  \quad \dots \quad I_q]_{q\times rq},\]
    \[Q_1 =\dots =Q_r = \hat{\boldsymbol{\pi}}\hat{\boldsymbol{\pi}}^\prime.\] $\hat{\boldsymbol{\pi}}$ is the matrix of $q \times |\mathcal{T}_{\textrm{window}}|$ that contains base probabilistic forecasts in the rolling window. If \[A = \left[\begin{matrix}
      A_1' \\ \vdots \\ A_r'
    \end{matrix}\right],\] then $\textrm{vec}(A) = [A_1' \cdots A_r']'$.

    
    \subsection{Computational considerations}
    
    As $m$, $n$ and $d_i$ increase $q$ and $r$ grow exponentially resulting in too many $a_{kj}$. An idea to simplify the optimisation is to force $a_{kj}=0$ if 
    \[
     c_{kj}>\underset{j}{\min}\,c_{kj}\,.
    \]  
    This implies that probability can only be moved to one of the nearest points. In the three variable example it implies probability can be moved from $(0,1,0)'$ to $(0,0,0)$ and $(0,1,1)$ but not to $(1,0,1)$ and $(1,1,2)$. It also implies that $a_{kj}=1$ for all $k,j$ such that $\hat{h}(k)=\tilde{h}(j)$. In words, all probability from a coherent point is assigned to the same coherent point.     
    
    
\end{document}