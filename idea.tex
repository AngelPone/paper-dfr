\documentclass[a4paper,review,11pt,authoryear]{elsarticle}

\usepackage{natbib}
\usepackage{amsfonts,amsmath,bm,xcolor}
\usepackage[ruled]{algorithm2e}
\usepackage{geometry}
\geometry{a4paper,scale=0.8}
\usepackage{setspace}
\setstretch{1.5}
\let\code=\texttt
\let\proglang=\textsf
\title{Discrete Forecast Reconciliation}


\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\author{Anastasios Panagiotelis, Bohan Zhang, Yanfei Kang, Feng li}

\begin{document}

	\maketitle
	\section{Preliminaries}
	Let $\bY=\left(Y_1,Y_2,\ldots,Y_n\right)'$ be an $n$-vector of discrete random variables. Let $\bY$ be partitioned so that the first $m$ elements are `bottom-level' variables the remaining $(n-m)$ variables are `top-level' series. The `top-level' variables are some deterministic linear function of the `bottom-level' variables, usually aggregates. Let the domain of each element of $\bY$ be given by $\mathcal{D}(Y_i)=\left\{0, 1,2,3,\dots,D_i\right\}$, where $D_i$ is finite.
	
	\section{Incoherent and Incoherent Domain}
	The \textit{incoherent domain} of $\bY$ is given by
	\[
	\hat{\mathcal D}(\bY)=\prod\limits_{i=1}^n\hat{\mathcal D}(\bY_i)=\left\{0, 1,2,\dots,D_1\right\}\times\left\{0,1,2,\dots,D_2\right\}\times\dots\times\left\{0,1,2,\dots,D_n\right\}\,,
	\] 
    where products are set products. The cardinality of the incoherent domain $|\hat{\mathcal D}(\bY)|=\prod\limits_{i=1}^{n} (D_i+1)$ which will be denoted by $q$. The incoherent domain is analogous to $\mathbb{R}^n$ is the continuous case.
    
    The \textit{coherent domain} of $\bY$ is given by a subset of $\tilde{\mathcal D}(\bY)$ for which aggregation constraints hold.  The coherent domain has cardinality $|\tilde{\mathcal D}(\bY)|=\prod\limits_{i=1}^{m} (D_i+1)$. This will be denoted as $r$.  The coherent domain is analogous to the coherent subspace $\mathfrak{s}$ in the continuous case.
    
    \subsubsection*{Example}
    
    Let $Y_1$ and $Y_2$ be binary variables and $Y_3=Y_1+Y_2$. In this case the domain of each variable is
    \begin{align*}
      \mathcal{D}(Y_1)=&\left\{0,1\right\}\\
      \mathcal{D}(Y_2)=&\left\{0,1\right\}\\
      \mathcal{D}(Y_3)=&\left\{0,1,2\right\}\,,
    \end{align*}	
    the incoherent domain is
    \begin{align*}
    \hat{\mathcal D}(\bY)=&\left\{\textcolor{blue}{(0,0,0)'},(0,1,0)',(1,0,0)',(1,1,0)',\right.\\
    &\left.(0,0,1)',\textcolor{blue}{(0,1,1)'},\textcolor{blue}{(1,0,1)'},(1,1,1)',\right.\\
    &\left.(0,0,2)',(0,1,2)',(1,0,2)',\textcolor{blue}{(1,1,2)'}\right\}\,,
    \end{align*}
    and the coherent domain is given by those points for which $y_1+y_2=y_3$, highlighted in blue above, i.e
    \[
        \tilde{\mathcal D}(\bY)=\left\{(0,0,0)',(0,1,1)',(1,0,1)',(1,1,2)'\right\}\,.
    \]
    
    \section{Probabilistic Forecasts}
    Suppose a probabilistic $h$-step ahead forecast is made for $\bY$ at time $t$. Let $\hat{\bpi}^{t+h|t}$ be a $q$-vector of probabilities where each element in $\hat{\bpi}^{t+h|t}$ corresponds to one point in the incoherent domain. Two notational conventions will be used for the elements of $\hat{\bpi}^{t+h|t}$. First, $\hat{\pi}_j^{t+h|t}$ will be used to denote the $j^{th}$ element of $\hat{\bpi}^{t+h|t}$.  Second, $\hat{\pi}_{(y_1 y_2 \dots y_n)}^{t+h|t}$ will be used to denote that the specific element of $\hat{\bpi}^{t+h|t}$, corresponds to the forecast probability that the $\bY$ takes a value $(y_1,y_2,\dots,y_n)'$. The two conventions are linked by a function $\hat{h}:{1,2,\dots,q}\rightarrow\hat{\mathcal{D}}(\bY)$ which maps each index $j$ to a configuration of values that $\bY$ can take. Using the small example in the previous section $\hat{\pi}_1^{t+h|t}=\hat{\pi}_{(000)}^{t+h|t}$, $\hat{\pi}_2^{t+h|t}=\hat{\pi}_{(010)}^{t+h|t}$, etc., and $\hat{h}(1)=(0,0,0)'$, $\hat{h}(2)=(0,1,0)'$, etc.
    
    This probabilistic forecast is incoherent as long as there is some $\hat{\bpi}^{t+h|t}_j\neq 0$ for which $\hat{h}(j)\notin\tilde{\mathcal{D}}(\bY)$, i.e. probability is assigned to some point for which the aggregation constraints do not hold. This can happen quite easily in practice. Suppose probabilistic forecasts are generated for each variable independently and the joint forecast is then constructed assuming independence. Such forecasts will in general be incoherent. For example suppose $Pr(Y_1=0)=0.2$,$Pr(Y_2=1)=0.1$,$Pr(Y_3=0)=0.05$, then under independence $Pr(Y_1=0,Y_2=1,Y_3=0)=0.2\times0.1\times0.05$. We are assigning non-zero probability to an incoherent point.
    
    A coherent probabilistic function can be defined in a similar fashion. Now $\tilde{\bpi}^{t+h|t}$ is an $r$-vector of probabilities where each element in $\tilde{\bpi}^{t+h|t}$ corresponds to one point in the coherent domain. The notation $\tilde{\pi}_k^{t+h|t}$ is used to denote the $k^{th}$ element of $\tilde{\bpi}^{t+h|t}$ and the notation $\tilde{\pi}_{(y_1 y_2 \dots y_n)}^{t+h|t}$ will be used to denote that the specific element of $\tilde{\bpi}^{t+h|t}$, corresponds to the forecast probability that the $\bY$ takes a value $(y_1,y_2,\dots,y_n)'$ where this value must be coherent. The analogue to $\hat{h}(j)$ is a function a function $\tilde{h}:{1,2,\dots,r}\rightarrow\tilde{\mathcal{D}}(\bY)$ which maps each index $k$ to a coherent configuration of values that $\bY$ can take. Using the small example in the previous section $\tilde{\pi}_1^{t+h|t}=\tilde{\pi}_{(000)}^{t+h|t}$, $\tilde{\pi}_2^{t+h|t}=\tilde{\pi}_{(011)}^{t+h|t}$, etc., and $\tilde{h}(1)=(0,0,0)'$, $\tilde{h}(2)=(0,1,1)'$, etc.
    
    \subsubsection*{Example}
    
    Consider the earlier example, with binary $y_1$ and $y_2$ and $y_1+y_2=y_3$. The incoherent probabilistic forecast is given by
    \[
      \hat{\bpi}^{t+h|t}=\begin{pmatrix}
         \hat{\pi}^{t+h|t}_{(000)}\\
         \hat{\pi}^{t+h|t}_{(010)}\\
         \hat{\pi}^{t+h|t}_{(100)}\\
         \hat{\pi}^{t+h|t}_{(110)}\\
         \hat{\pi}^{t+h|t}_{(001)}\\
         \hat{\pi}^{t+h|t}_{(011)}\\
         \hat{\pi}^{t+h|t}_{(101)}\\
         \hat{\pi}^{t+h|t}_{(111)}\\
         \hat{\pi}^{t+h|t}_{(002)}\\
         \hat{\pi}^{t+h|t}_{(012)}\\
         \hat{\pi}^{t+h|t}_{(102)}\\
         \hat{\pi}^{t+h|t}_{(112)}\\
      \end{pmatrix}
    \]
    where the notation $\hat{\pi}^{t+h|t}_{1}$ can be used instead of $\hat{\pi}^{t+h|t}_{(000)}$, $\hat{\pi}^{t+h|t}_{2}$ can be used instead of $\hat{\pi}^{t+h|t}_{(010)}$, etc. Also the function $\hat{h}$ is defined so that $\hat{h}(1)=(0,0,0)'$, $\hat{h}(2)=(0,1,0)'$, etc.
    
    The coherent probabilistic forecast is given by
    \[
    \tilde{\bpi}^{t+h|t}=\begin{pmatrix}
    \tilde{\pi}^{t+h|t}_{(000)}\\
    \tilde{\pi}^{t+h|t}_{(011)}\\
    \tilde{\pi}^{t+h|t}_{(101)}\\
    \tilde{\pi}^{t+h|t}_{(112)}\\
    \end{pmatrix}
    \]
    where the notation $\tilde{\pi}^{t+h|t}_{1}$ can be used instead of $\tilde{\pi}^{t+h|t}_{(000)}$, $\tilde{\pi}^{t+h|t}_{2}$ can be used instead of $\tilde{\pi}^{t+h|t}_{(011)}$, etc. Also the function $\tilde{h}$ is defined so that $\tilde{h}(1)=(0,0,0)'$, $\tilde{h}(2)=(0,1,1)'$, etc. Note that the ordering of the probabilities and the functions $\hat{h}$ and $\tilde{h}$ are not unique although this does not affect the proposed algorithms.
    
    \section{Reconciliation}
    
    The idea is to `move' probability from $\hat{\bpi}$ to $\tilde{\bpi}$ with the superscript $t+h|t$ dropped for convenience.  Let
    
    \[
    \tilde{\bpi}=\bm{A}\hat{\bpi}
    \] 
    
    where $\bm{A}$ is a matrix of assignment weights. Letting, $a_{kj}$ be the element in row $k$ and column $j$ of $\bm{A}$, this is equivalent to
    \[
      \tilde{\pi}_k=\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j
    \]
    for all $k$. Each $a_{kj}$ tells us how much probability is shifted from the (possibly) incoherent point $\hat{h}(j)$ to the coherent point $\tilde{h}(k)$ (or from element $j$ in $\hat{\bpi}$ to element $k$ in $\hat{\bpi}$). We know that $a_{kj}$ must meet the following constraints
    \begin{align*}
    &0\leq a_{kj} \leq 1 \,\forall k, j\\ 
    &\sum\limits_{k=1}^r a_{kj} = 1 \,\forall j 
    \end{align*}
    The first constraint guarantees that the elements of $\tilde{\bpi}$ are between 0 and 1, while the second constraint guarantees that the elements of $\tilde{\bpi}$ sum to 1.
    
    The next thing to consider is what would make a `good' candidate for $\bm{A}$.  One property is that that we move probability from incoherent points to coherent points that are  `nearby' in some sense. Another is that the resulting coherent forecasts perform well.  Since we are using probabilistic forecasts some scoring rule can be used to measure performance.
    
    \subsection{Costs}
    
    The idea of moving probability to nearby coherent points is inspired by the use (and success) of projections in the reconciliation literature for continuous forecasts. We define a `cost' of moving probability from $\hat{\pi}_j\rightarrow\tilde{\pi}_k$ as
    \[
    c_{kj}=||\hat{h}(j)-\tilde{h}(k)||_1\,,
    \]
    where $||.||_1$ is the L1 norm. To illustrate a single cost in the three variable example is given by
    \[
    c_{12}=\left|\left|\begin{pmatrix}0\\1\\0\end{pmatrix}-\begin{pmatrix}0\\0\\0\end{pmatrix}\right|\right|_1=1\,.
    \]
    Note that in contrast to the continuous case the `nearest' coherent point is not unique for example $(0,1,0)'$ is equally distant from $(0,0,0)'$ and $(0,1,1)'$ both of which are coherent. For this reason we cannot base discrete reconciliation on a discrete analogue to projections alone.
    
    \subsection{Brier Score}
    
    Assume that $\hat{\bpi}^{t+h|t}$ are found for $t\in\mathcal{T}_{\textrm{window}}$, were $\mathcal{T}_{\textrm{window}}$ is a rolling window. Also, let $z^{t+h}$ be an $r$-vector with element $k=1$ if $\tilde{h}(k)=\bm{y}^{t+h}$ and $0$ otherwise where $\bm{y}^{t+h}$ is the actual realisation of $Y$ at time $t+h$. The Brier score is  proper scoring rule and can be averaged over the rolling window as
    \begin{align*}
    \textrm{Av. Brier Sc.}=&\frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\tilde{\pi}_k^{t+h|t}-z^{t+h}_k\right)^2\right]\\
    =&\frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j-z^{t+h}_k\right)^2\right]\,.
    \end{align*}
    This is a quadratic function of the $a_{kj}$ with smaller values indicating a better coherent forecast.
    
    \section{The optimisation problem}
    
    Putting everything together, the optimisation problem is to 
    
    \[
    \underset{a_{kj}}{\min} \frac{1}{|\mathcal{T}_{\textrm{window}}|}\sum\limits_{\mathcal{T}_{\textrm{window}}}\left[\sum\limits_{k=1}^r\left(\sum\limits_{j=1}^q a_{kj}\hat{{\pi}}_j-z^{t+h}_k\right)^2\right] + \sum\limits_{k=1}^r\sum\limits_{j=1}^q c_{kj}a_{kj}\,
    \]
    subject to
    \begin{align*}
    &0\leq a_{kj}\leq 1\\
    &\sum\limits_{k=1}^r a_{kj} = 1 \,\forall j
    \end{align*}
    This is a fairly standard problem in quadratic programming.

    \subsection{Matrix form of the optimisation problem}

    The optimisation priblem can be formulated into matrix form.
    \begin{align*}
      \min\limits_{A \in \mathbb{R}^{r\times q}} \frac{1}{|\mathcal{T}_{\textrm{window}}|}\text{vec}(A)' Q \text{vec}(A) &+ \left[\lambda \text{vec}(c)' - \frac{2}{|\mathcal{T}_{\textrm{window}}|}\text{vec}(Z)'D\right]\text{vec}(A) \\&+ \frac{1}{|\mathcal{T}_{\textrm{window}}|} \text{vec}(Z)'\text{vec}(Z),
    \end{align*}
    subject to \begin{align*}
      0 \leq A \leq 1\\
      E A = \mathbf{1}
    \end{align*}
    where
    \[
    Q = \left[\begin{matrix}
      Q_1  & \\
       &\ddots \\
       &  & Q_r

    \end{matrix}  \right]
    D = \left[\begin{matrix}
      D_1 \\ \vdots \\ D_T
    \end{matrix}\right] 
    D_t = \left[\begin{matrix}
      \hat\pi_{t}' & \\
      & \ddots & \\
      & & \hat\pi_{t}'
    \end{matrix}\right]_{r\times (rq)}
    \]

    and
    \[\textrm{vec}(c) = \left[c_1' \dots c_r'\right]',\]
    \[\textrm{vec}(Z) = \left[Z_1' \dots Z_{|\mathcal{T}_{\textrm{window}}|}'\right]',\]
    \[E = [I_q \quad I_q  \quad \dots \quad I_q]_{q\times rq},\]
    \[Q_1 =\dots =Q_r = \hat{\boldsymbol{\pi}}\hat{\boldsymbol{\pi}}^\prime.\] $\hat{\boldsymbol{\pi}}$ is the matrix of $q \times |\mathcal{T}_{\textrm{window}}|$ that contains base probabilistic forecasts in the rolling window. If \[A = \left[\begin{matrix}
      A_1' \\ \vdots \\ A_r'
    \end{matrix}\right],\] then $\textrm{vec}(A) = [A_1' \cdots A_r']'$.    

    
    \subsection{Computational considerations}
    
    As $m$, $n$ and $d_i$ increase $q$ and $r$ grow exponentially resulting in too many $a_{kj}$. An idea to simplify the optimisation is to force $a_{kj}=0$ if 
    \[
     c_{kj}>\underset{j}{\min}\,c_{kj}\,.
    \]  
    This implies that probability can only be moved to one of the nearest points. In the three variable example it implies probability can be moved from $(0,1,0)'$ to $(0,0,0)$ and $(0,1,1)$ but not to $(1,0,1)$ and $(1,1,2)$. It also implies that $a_{kj}=1$ for all $k,j$ such that $\hat{h}(k)=\tilde{h}(j)$. In words, all probability from a coherent point is assigned to the same coherent point.

   \subsection{Step-wise reconciliation}

   Although forcing corresponding parameters of the non-nearest points to 0 reduce the dimension, there can still be a large amount of unknown parameters to estimate when handling high-dimensional hierarchy. 
   
   Consider a two-hierarchy with one root node and $k$ children nodes. 
   The children nodes have the same domain $\{0, 1\}$, and thus the root node hierarchies domain $\{0, 1, \dots, k\}$.
   The cardinality of coherent domain would be $2^k$, and the cardinality of incoherent domain would be $2^k\cdot (k+1)$. 
   The dimension of the parameter space of $A$ would be at least $2^k$, when we simply assume each incoherent point has two nearest neighbors. Actually, there may be $k$ nearest neighbors and even more. For example, when $k=4$, the nearest neighbors of $00001$ contain $00000, 10001, 01001, 00101, 00011$.

   By destructing the high-dimensional hierarchy into multiple simplest two-level hierarchies with three nodes and averaging the reconciled marginal distribution, we can avoid the problem caused by exponentially growth of the parameter space. 
  

   Denote the root node as $y_0$ and children nodes as $y_1, \dots, y_k$. 
   Denote the marginal distribution of node $y_1$ as $p(y_1)$ and the joint distribution of node $y_1$ and $y_2$ as $p(y_1, y_2)$. 
   Denote the reconciled marginal distribution of $y_1$ as $\tilde p(y_1)$.

   \begin{algorithm}[H]
    \caption{Step-wise reconciliation}
    \SetKwFunction{reconcile}{reconcile}
    \SetKwFunction{calculus}{calculus}
    \SetKwFunction{alignDistribution}{alignDistribution}
    \SetKwFunction{calJointDistribution}{calJointDistribution}
    \SetKwFunction{forecast}{forecast}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{Time series $\bm{y} = (y_1,\dots,y_k)'$}
    $y_{U_1} \leftarrow \sum\limits_{j=2}^k y_j$\;
    $y_T \leftarrow y_1 + y_{U_1}$ \;
    $\hat p(y_1) \leftarrow$ \forecast{$y_1$}\;
    $\hat p(y_{U_1}) \leftarrow$ \forecast{$y_{U_1}$}\;
    $\hat p(y_{T}) \leftarrow$ \forecast{$y_{T}$}\;
    $\tilde p(y_1, y_{U_1}) \leftarrow $ \reconcile{$\hat p_1(y_1)$, $\hat p_1(y_{U_1})$,$\hat p_1(y_T)$}\;
    $\tilde p_1(y_{U_1}) \leftarrow$ \calculus{$\tilde p_1(y_1, y_{U_1})$}\;
    \For {$i=2,\dots,k-1$}{
      $y_{U_{i}} \leftarrow \sum\limits_{j=i+1}^k y_j$\;
      $\hat p(y_i) \leftarrow$ \forecast{$y_i$}\;
      $\hat p(y_{U_i}) \leftarrow$ \forecast{$y_{U_i}$}\;
      $\tilde p(y_i, y_{U_i}) \leftarrow $ \reconcile{$\hat p(y_i)$, $\hat p(y_{U_i})$, $\hat p(y_{U_{i-1}})$} \;
      $\tilde p_i(y_{U_{i-1}}) \leftarrow$ \calculus{$\tilde p(y_i, y_{U_i})$} \; 
      $\tilde p(y_{U_{i-1}}) \leftarrow \frac{1}{2}(\tilde p_{i-1}(y_{U_{i-1}}) + \tilde p_i(y_{U_{i-1}}))$\;
      $\tilde p'(y_i, y_{U_i}) \leftarrow$ \alignDistribution{$\tilde p(y_{U_{i-1}}), \tilde p(y_i, y_{U_i})$}\;
      $\bm{y}_{i-1} \leftarrow (y_1,\dots,y_{i-1})'$\;
      $\tilde p'(\bm{y}_{i-1}, y_{U_{i-1}}) \leftarrow$ \alignDistribution{$\tilde p(y_{U_{i-1}}), \tilde p(\bm{y}_{i-1}, y_{U_{i-1}})$}\;
      $\tilde p(\bm{y}_{i-1}, y_{i}, y_{U_i}) \leftarrow$ \calJointDistribution{$\tilde p'(\bm{y}_{i-1}, y_{U_{i-1}}), \tilde p'(y_i, y_{U_i}), \tilde p(y_{U_{i-1}})$}\;
    
    }
    \Output{$\tilde p(\bm{y})$}
    
   \end{algorithm}

   By reordering $\bm{y}$, we can obtain multiple $\tilde p(\bm{y})$ and average them to obtain final reconciled joint distribution.

  Given reconciled joint distribution $p(A, B)$ and $p(C,D)$, where $B=C+D$, we can obtain joint distribution of $A,C,D$ by assuming $C,D \independent{A}$:

  \[
      \begin{aligned}
        p(A,B,C,D) &= p(C, D|A,B)p(A,B) \\ 
          &= p(C,D|B)p(A,B)  \:\: \textrm{assume}\:\: C,D \independent {A}\\
          &= \frac{p(B,C,D)}{p(B)}p(A,B) \\
          &= \frac{p(C,D)}{p(B)}p(A,B);
     \end{aligned}
     \]
     \[
       p(A,C,D) = \int P(A,B,C,D) \textrm{d}B.
      \]


   When handling hierarchy with more aggregated series, we can repeat above procedure for each aggregated series. 
   Or we can repeat the above procedure in a top-down or bottom-up manner, which means that we reconcile the base forecasts of one level and use the reconciled forecasts as base children forecasts(bottom-up) or base root forecasts (top-down) to reconcile next level. One drawback of discussed approaches is that they can not consider the hierarchy as a whole.

    
\section{Simulation}

To demonstrate the usefulness of the proposed framework, we conduct two simulation experiments in different context. 
In the first cross-sectional setting, we use the simplest three-node hierarchy. 
While in the temporal setting, we simulate daily time series and use eight-node hierarchy.

\subsection{Cross-sectional hierarchy}

\subsection{Temporal hierarchy}

We now consider a daily temporal setting with one total level and $m=7$ bottom-level series. The bottom-level time series can only take values of $0$ and $1$. 
Even though, the domain of the hierarchy is still quite large, so it is challenging to estimate a full reconciliation matrix given limited observations in practice. 
To avoid the curse of dimensionality, we use the proposed step-wise reconciliation algorithm. 

\subsubsection{Simulation setup} 

The time series were simulated using function \code{simID} in package \code{tsintermittent} for \proglang{R}. 
The simulation algorithm is designed for simulating intermittent demand series, which is particularly suitable in our context.
We can control the algorithm by three parameters, which are average intermittent demand interval (\code{idi}), variation of the non-zero demands (\code{cv2}) and mean level of the non-zero demands (\code{level}) (\Citealp{petropoulosHorsesCoursesDemand2014}). We set \code{idi}$=2$, \code{level} $\:=1.5$ and \code{cv2} $\:=0.5$ in this experiment.
The simulated time series are not guaranteed to take only $0$ and $1$, so we set values bigger than one to be one. 

We repeated the simulation process $1000$ times. For each series, $T=827$ observations were generated; the final $h=21$ observations were used as test sets. 
In the training period, we employed the rolling origin strategy (\Citealp{hyndmanForecastingPrinciplesPractice2021}) with fixed lookback window size $k=300$. 
For each rolling timestamp, latest $300$ observations were taken as input to generate probabilistic forecasts in the next $7$ days. In total, we had $500$ windows, thus $500$ samples, used for training the step-wise reconciliation model.
Using the same strategy, there were $14$ samples in test period can be used to evaluate forecast performance.




// Simulation algorithm used by \code{tsintermittent}.


\bibliographystyle{agsm}
\bibliography{references.bib}

\end{document}